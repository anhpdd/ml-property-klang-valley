{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhpdd/ml-property-valuation-klang-valley/blob/main/notebooks/3_Merging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5276c471",
      "metadata": {
        "id": "5276c471"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q osmnx\n",
        "\n",
        "# Core libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "import networkx as nx\n",
        "\n",
        "# Geospatial\n",
        "import osmnx as ox\n",
        "from shapely.geometry import Point, LineString, Polygon, MultiPolygon\n",
        "from shapely import wkt\n",
        "from geopy.distance import great_circle\n",
        "\n",
        "# Visualization\n",
        "import folium\n",
        "from matplotlib.colors import Normalize, to_hex, LinearSegmentedColormap\n",
        "from branca.colormap import LinearColormap\n",
        "\n",
        "# Utilities\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "from typing import List, Dict, Any, Optional\n",
        "from IPython.display import display\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a821071",
      "metadata": {
        "id": "2a821071"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b40d5f7",
      "metadata": {
        "id": "6b40d5f7"
      },
      "outputs": [],
      "source": [
        "def extract_line_geometry(input_df: pd.DataFrame, name_column,id_column) -> Optional[gpd.GeoDataFrame]:\n",
        "    \"\"\"\n",
        "    Fetches OSM data for Way IDs found in the input DataFrame,\n",
        "    parses road details and geometry, and returns a single GeoDataFrame\n",
        "    containing all valid road LineStrings, merging with input DataFrame attributes.\n",
        "\n",
        "    Args:\n",
        "        input_df (pd.DataFrame): A DataFrame expected to have a 'Way ID' column\n",
        "                                  and any other attributes to carry over.\n",
        "\n",
        "    Returns:\n",
        "        Optional[gpd.GeoDataFrame]: A GeoDataFrame with road geometries and\n",
        "                                     merged attributes, or None if no valid data.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    all_geometries = []\n",
        "    all_attributes = []\n",
        "\n",
        "    for index, row in input_df.iterrows():\n",
        "        way_id = str(row[id_column]) # Ensure way_id is a string\n",
        "\n",
        "        url = f\"{OSM_API_BASE_URL}/way/{way_id}/full\"\n",
        "        root = fetch_osm_data(url)\n",
        "\n",
        "        if not root:\n",
        "            print(f\"Skipping Way ID '{way_id}' due to data fetch/parse error.\")\n",
        "            continue\n",
        "\n",
        "        road_way_elem = root.find(f\".//way[@id='{way_id}']\")\n",
        "        if not road_way_elem:\n",
        "            print(f\"Warning: Road Way ID '{way_id}' not found in fetched XML for {url}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Node caching for the *current* XML response\n",
        "        node_coords_cache = {} # {node_id: (lon, lat)}\n",
        "        for node_elem in root.findall(\".//node\"):\n",
        "            try:\n",
        "                node_id = node_elem.get('id')\n",
        "                lat = float(node_elem.get('lat'))\n",
        "                lon = float(node_elem.get('lon'))\n",
        "                node_coords_cache[node_id] = (lon, lat)\n",
        "            except (TypeError, ValueError):\n",
        "                pass # Skip invalid nodes\n",
        "\n",
        "        # Extract road attributes (tags) from OSM\n",
        "        osm_road_attrs = {'id': way_id}\n",
        "        for tag in road_way_elem.findall('tag'):\n",
        "            osm_road_attrs[tag.get('k')] = tag.get('v')\n",
        "\n",
        "        # Combine attributes from the input DataFrame row with OSM attributes\n",
        "        # The input DataFrame attributes will take precedence if keys overlap\n",
        "        merged_attrs = row.to_dict() # Start with attributes from the input row\n",
        "        merged_attrs.update(osm_road_attrs) # Add/overwrite with OSM-fetched attributes\n",
        "\n",
        "        # Build coordinates for the LineString\n",
        "        road_coords = []\n",
        "        for nd_ref_elem in road_way_elem.findall('nd'):\n",
        "            node_ref = nd_ref_elem.get('ref')\n",
        "            coords = node_coords_cache.get(node_ref)\n",
        "            if coords:\n",
        "                road_coords.append(coords)\n",
        "\n",
        "        # Create LineString if enough points exist\n",
        "        if len(road_coords) >= 2:\n",
        "            line_geometry = LineString(road_coords)\n",
        "            all_geometries.append(line_geometry)\n",
        "            all_attributes.append(merged_attrs)\n",
        "        else:\n",
        "            print(f\"Warning: Insufficient coordinates for Way ID '{way_id}' to form a LineString. Skipping.\")\n",
        "\n",
        "    if not all_geometries:\n",
        "        print(\"No valid LineString geometries could be created for any of the provided Way IDs.\")\n",
        "        return None\n",
        "\n",
        "    # Create a single GeoDataFrame from all collected geometries and attributes\n",
        "    # The 'crs' (Coordinate Reference System) specifies WGS84, common for GPS coordinates.\n",
        "    gdf = gpd.GeoDataFrame(all_attributes, geometry=all_geometries, crs=\"EPSG:4326\")\n",
        "    gdf = gdf.drop_duplicates(subset= name_column)\n",
        "    gdf = gdf[[name_column, 'id', 'geometry']]\n",
        "\n",
        "    return gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "884fd570",
      "metadata": {
        "id": "884fd570"
      },
      "outputs": [],
      "source": [
        "def visualize_gdf_on_folium_colored_with_legend(\n",
        "    gdf: gpd.GeoDataFrame,\n",
        "    columns_to_display: Optional[List[str]] = None,\n",
        "    residential_price_column: str = 'RM per Land m2',\n",
        "    top_percentile_highlight: float = 0.90\n",
        ") -> Optional[folium.Map]:\n",
        "    if gdf.empty or 'geometry' not in gdf.columns:\n",
        "        print(\"GeoDataFrame is empty or missing 'geometry' column.\")\n",
        "        return None\n",
        "\n",
        "    valid_lines_gdf = gdf[gdf.geometry.apply(lambda geom: isinstance(geom, LineString) and not geom.is_empty)].copy()\n",
        "    if valid_lines_gdf.empty:\n",
        "        print(\"No valid LineString geometries found.\")\n",
        "        return None\n",
        "\n",
        "    # --- Map Centering ---\n",
        "    try:\n",
        "    # Always convert to 4326 for centering and plotting\n",
        "      temp_gdf = valid_lines_gdf.to_crs(\"EPSG:4326\")\n",
        "      map_center_geom = temp_gdf.geometry.unary_union.centroid\n",
        "      m = folium.Map(\n",
        "          location=[map_center_geom.y, map_center_geom.x],\n",
        "          zoom_start=12,\n",
        "          tiles='CartoDB positron'  # <-- ADD THIS LINE\n",
        "      )\n",
        "    except Exception as e:\n",
        "        print(f\"Could not determine map center: {e}. Using default location.\")\n",
        "        m = folium.Map(\n",
        "            location=[3.1390, 101.6869],\n",
        "            zoom_start=10,\n",
        "            tiles='CartoDB positron'\n",
        "        )\n",
        "\n",
        "    # --- Price Normalization and Colormap ---\n",
        "    min_price_val, max_price_val, top_percentile_threshold, norm_normal_range = None, None, None, None\n",
        "    colors = ['green', 'yellow', 'orange']\n",
        "    normal_residential_cmap = LinearSegmentedColormap.from_list('GnYeOr', colors)\n",
        "    top_percentile_color_hex = to_hex('red')\n",
        "\n",
        "    if residential_price_column not in valid_lines_gdf.columns:\n",
        "        print(f\"Error: Price column '{residential_price_column}' not found.\")\n",
        "        return m # Return map without colored lines\n",
        "\n",
        "    prices_numeric = pd.to_numeric(valid_lines_gdf[residential_price_column], errors='coerce').dropna()\n",
        "    if len(prices_numeric) > 1:\n",
        "        top_percentile_threshold = prices_numeric.quantile(top_percentile_highlight)\n",
        "        normal_range_prices = prices_numeric[prices_numeric < top_percentile_threshold]\n",
        "        if not normal_range_prices.empty:\n",
        "            min_price_val = normal_range_prices.min()\n",
        "            max_price_val = normal_range_prices.max()\n",
        "            norm_normal_range = Normalize(vmin=min_price_val, vmax=max_price_val) if min_price_val != max_price_val else Normalize(vmin=min_price_val, vmax=min_price_val + 1)\n",
        "\n",
        "    # --- FIX 1: Reproject data to the correct CRS for Folium ---\n",
        "    valid_lines_gdf = valid_lines_gdf.to_crs(\"EPSG:4326\")\n",
        "\n",
        "    # --- Main Loop to Draw Lines ---\n",
        "    for idx, row in valid_lines_gdf.iterrows():\n",
        "        line = row['geometry']\n",
        "        folium_coords = [(point[1], point[0]) for point in line.coords]\n",
        "\n",
        "        line_color = 'grey' # Default color\n",
        "        current_price = pd.to_numeric(row.get(residential_price_column), errors='coerce')\n",
        "\n",
        "        if pd.notna(current_price):\n",
        "            if top_percentile_threshold and current_price >= top_percentile_threshold:\n",
        "                line_color = top_percentile_color_hex\n",
        "            elif norm_normal_range:\n",
        "                line_color = to_hex(normal_residential_cmap(norm_normal_range(current_price)))\n",
        "\n",
        "        folium.PolyLine(locations=folium_coords, color=line_color, weight=4, opacity=0.8).add_to(m)\n",
        "\n",
        "    # --- FIX 2: Complete legend generation code ---\n",
        "    legend_html = \"\"\"\n",
        "    <div style=\"position: fixed; bottom: 50px; left: 50px; width: auto; height: auto;\n",
        "                border:2px solid grey; z-index:9999; font-size:14px;\n",
        "                background-color:white; opacity:0.9; padding: 10px;\">\n",
        "        <b>Legend</b> <br>\n",
        "    \"\"\"\n",
        "    if top_percentile_threshold is not None:\n",
        "        legend_html += f'<i style=\"background:{top_percentile_color_hex}; opacity:0.8; border: 1px solid #000; display: inline-block; width: 12px; height: 12px; margin-right: 5px;\"></i> Top {100*(1-top_percentile_highlight):.0f}% Price (≥ {top_percentile_threshold:,.0f})<br>'\n",
        "    if norm_normal_range and min_price_val is not None and max_price_val is not None:\n",
        "        legend_html += f'<b>Price (Normal Range)</b><br>'\n",
        "        gradient_html = '<div style=\"background-image: linear-gradient(to right, green, yellow, orange); height: 20px; width: 100%; border: 1px solid #ccc;\"></div>'\n",
        "        legend_html += f'<div style=\"display: flex; justify-content: space-between;\"><span>Low: {min_price_val:,.0f}</span><span>High: {max_price_val:,.0f}</span></div>'\n",
        "        legend_html += gradient_html\n",
        "    else:\n",
        "        legend_html += '<i style=\"background:grey; opacity:0.8; border: 1px solid #000; display: inline-block; width: 12px; height: 12px; margin-right: 5px;\"></i> Residential (Price N/A or uniform)<br>'\n",
        "    legend_html += \"</div>\"\n",
        "    m.get_root().html.add_child(folium.Element(legend_html))\n",
        "\n",
        "    return m"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee4db707",
      "metadata": {
        "id": "ee4db707"
      },
      "source": [
        "# Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eJQekMvOzMAB",
      "metadata": {
        "id": "eJQekMvOzMAB"
      },
      "outputs": [],
      "source": [
        "# Import property dataset\n",
        "df_raw=pd.read_excel('/content/drive/MyDrive/Colab/Capstone 1/tprop_df_validated.xlsx')\n",
        "\n",
        "# Inspect dataset\n",
        "#df_raw.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bGNSAUG-08la",
      "metadata": {
        "id": "bGNSAUG-08la"
      },
      "outputs": [],
      "source": [
        "# Inspect datatypes\n",
        "#df_raw.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nXW_JUy5zgxb",
      "metadata": {
        "id": "nXW_JUy5zgxb"
      },
      "outputs": [],
      "source": [
        "df = df_raw.copy()\n",
        "\n",
        "# Convert the date column\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "# Fix property size\n",
        "df['property_m2'] = df['property_m2'] / 100\n",
        "df['land_m2'] = df['land_m2'] / 100\n",
        "\n",
        "# Binary tenure feature\n",
        "df['freehold'] = (df['tenure'] == 'Freehold').astype(int)\n",
        "df['price_m2'] = round(df['transaction_price'] / df['property_m2'], 2)\n",
        "\n",
        "# Standardize strings\n",
        "string_cols = df.select_dtypes(include=['object']).columns\n",
        "string_cols = string_cols.drop('geometry', errors='ignore')\n",
        "for col in string_cols:\n",
        "    df[col] = df[col].str.strip().str.lower()\n",
        "\n",
        "\n",
        "# Convert string geometries to actual Shapely objects\n",
        "df['geometry'] = df['geometry'].apply(wkt.loads)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "cols_to_drop = ['tenure']\n",
        "df_clean = df.drop(columns=cols_to_drop, errors='ignore')\n",
        "\n",
        "# Verify the new dtypes\n",
        "#print(df_clean.info())\n",
        "#df_clean.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46c997d6",
      "metadata": {
        "id": "46c997d6"
      },
      "source": [
        "# Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "575039d0",
      "metadata": {
        "id": "575039d0"
      },
      "outputs": [],
      "source": [
        "# Create the GeoDataFrame and set the CRS\n",
        "gdf = gpd.GeoDataFrame(df_clean, geometry='geometry').set_crs(epsg=4326, inplace=True)\n",
        "\n",
        "# Configure the map\n",
        "#map_viz = visualize_gdf_on_folium_colored_with_legend(\n",
        "#    gdf,\n",
        "#    residential_price_column='price_m2'\n",
        "#)\n",
        "\n",
        "# Display the map\n",
        "#map_viz"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd01718d",
      "metadata": {
        "id": "fd01718d"
      },
      "source": [
        "# Amnity ID Extract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89c85434",
      "metadata": {
        "id": "89c85434"
      },
      "outputs": [],
      "source": [
        "# Import amenity data\n",
        "amenity_df = pd.read_excel('/content/drive/MyDrive/Colab/Capstone 1/all_pois_gdf.xlsx').drop(columns='Unnamed: 0').rename(columns = {'Station ID': 'station_id'})\n",
        "\n",
        "# Set up geometry\n",
        "amenity_df['geometry'] = amenity_df['geometry'].apply(wkt.loads)\n",
        "\n",
        "# Convert it to GeoDataFrame and set crs to 4326\n",
        "amenity_gdf = gpd.GeoDataFrame(amenity_df, geometry='geometry').set_crs(epsg=4326, inplace=True)\n",
        "\n",
        "#Inspect df\n",
        "#amenity_gdf.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import ridership data\n",
        "ridership = pd.read_excel('/content/drive/MyDrive/Colab/Capstone 1/ridership_data.xlsx')\n",
        "\n",
        "# Set up date\n",
        "ridership['date'] = pd.to_datetime(ridership['date'])\n",
        "\n",
        "# Clean station ID column\n",
        "ridership['station_id'] = ridership['station_name'].str.split(':').str[0]\n",
        "\n",
        "# Inspect dataset\n",
        "#ridership.head()"
      ],
      "metadata": {
        "id": "p3Y4IS7lA2t1"
      },
      "id": "p3Y4IS7lA2t1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect dataset datatypes\n",
        "#ridership.info()"
      ],
      "metadata": {
        "id": "Jd5bs_NFKFhb"
      },
      "id": "Jd5bs_NFKFhb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Count"
      ],
      "metadata": {
        "id": "HIJOFJmJEeRv"
      },
      "id": "HIJOFJmJEeRv"
    },
    {
      "cell_type": "code",
      "source": [
        "def count_amenities(\n",
        "    property_gdf: gpd.GeoDataFrame,\n",
        "    amenity_gdf: gpd.GeoDataFrame,\n",
        "    radius_km: float,\n",
        "    amenity_type_column: str = 'feature_type',\n",
        "    property_id_col: str = None,\n",
        "    amenity_id_col: str = None,\n",
        "    train_type_name: str = 'train'\n",
        ") -> gpd.GeoDataFrame:\n",
        "    \"\"\"\n",
        "    Buffer amenities and find which properties they affect - with auto-detection!\n",
        "    \"\"\"\n",
        "    result_gdf = property_gdf.copy()\n",
        "\n",
        "    # Auto-detect property ID column if not specified\n",
        "    if property_id_col is None:\n",
        "        # Look for common ID column names\n",
        "        possible_names = ['id', 'ID', 'property_id', 'prop_id', 'index']\n",
        "        for name in possible_names:\n",
        "            if name in property_gdf.columns:\n",
        "                property_id_col = name\n",
        "                break\n",
        "\n",
        "        if property_id_col is None:\n",
        "            # Use the index\n",
        "            print(\"No ID column found, using index as property ID\")\n",
        "            property_gdf = property_gdf.reset_index()\n",
        "            property_id_col = 'index'\n",
        "            result_gdf = property_gdf.copy()\n",
        "\n",
        "    print(f\"Using property ID column: '{property_id_col}'\")\n",
        "\n",
        "    # Auto-detect amenity ID column if not specified\n",
        "    if amenity_id_col is None:\n",
        "        possible_names = ['osm_id', 'id', 'ID', 'amenity_id']\n",
        "        for name in possible_names:\n",
        "            if name in amenity_gdf.columns:\n",
        "                amenity_id_col = name\n",
        "                break\n",
        "\n",
        "        if amenity_id_col is None:\n",
        "            # Create a temporary ID column\n",
        "            print(\"No amenity ID column found, creating temporary IDs\")\n",
        "            amenity_gdf = amenity_gdf.copy()\n",
        "            amenity_gdf['temp_amenity_id'] = range(len(amenity_gdf))\n",
        "            amenity_id_col = 'temp_amenity_id'\n",
        "\n",
        "    print(f\"Using amenity ID column: '{amenity_id_col}'\")\n",
        "\n",
        "    # Match CRS\n",
        "    if property_gdf.crs != amenity_gdf.crs:\n",
        "        print(\"Converting CRS to match...\")\n",
        "        amenity_gdf = amenity_gdf.to_crs(property_gdf.crs)\n",
        "\n",
        "    # Buffer AMENITIES with progress bar\n",
        "    print(f\"\\nCreating {radius_km}km buffers around {len(amenity_gdf)} amenities...\")\n",
        "    radius_degrees = radius_km / 111.0\n",
        "    amenity_buffered = amenity_gdf.copy()\n",
        "\n",
        "    tqdm.pandas(desc=\"Buffering amenities\")\n",
        "    amenity_buffered['geometry'] = amenity_buffered['geometry'].progress_apply(\n",
        "        lambda geom: geom.buffer(radius_degrees)\n",
        "    )\n",
        "\n",
        "    # Spatial join: which properties does each amenity affect?\n",
        "    print(\"\\nFinding affected properties (spatial join)...\")\n",
        "    affected = gpd.sjoin(\n",
        "        amenity_buffered[[amenity_id_col, amenity_type_column, 'geometry']],\n",
        "        property_gdf[[property_id_col, 'geometry']],\n",
        "        how='inner',\n",
        "        predicate='intersects'\n",
        "    )\n",
        "\n",
        "    print(f\"✓ Found {len(affected)} amenity-property pairs\")\n",
        "\n",
        "    # Initialize result columns\n",
        "    unique_types = amenity_gdf[amenity_type_column].dropna().unique()\n",
        "    print(f\"\\nAmenity types found: {', '.join(str(t) for t in unique_types)}\")\n",
        "\n",
        "    for amenity_type in unique_types:\n",
        "        type_name = str(amenity_type).lower().replace(' ', '_')\n",
        "        result_gdf[f\"{type_name}_count\"] = 0\n",
        "\n",
        "    result_gdf[\"total_amenity_count\"] = 0\n",
        "    result_gdf[\"train_ids\"] = \"\"\n",
        "\n",
        "    if len(affected) > 0:\n",
        "        # Count amenities by type for each property\n",
        "        print(\"\\nCounting amenities by type...\")\n",
        "        counts = affected.groupby([property_id_col, amenity_type_column]).size().unstack(fill_value=0)\n",
        "\n",
        "        # Map counts back to result with progress bar\n",
        "        print(\"Mapping counts to properties...\")\n",
        "        for amenity_type in tqdm(counts.columns, desc=\"Processing types\"):\n",
        "            type_name = str(amenity_type).lower().replace(' ', '_')\n",
        "            col_name = f\"{type_name}_count\"\n",
        "            if col_name in result_gdf.columns:\n",
        "                result_gdf[col_name] = result_gdf[property_id_col].map(\n",
        "                    counts[amenity_type].to_dict()\n",
        "                ).fillna(0).astype(int)\n",
        "\n",
        "        # Total count per property\n",
        "        print(\"Calculating total counts...\")\n",
        "        total = affected.groupby(property_id_col).size()\n",
        "        result_gdf['total_amenity_count'] = result_gdf[property_id_col].map(\n",
        "            total.to_dict()\n",
        "        ).fillna(0).astype(int)\n",
        "\n",
        "        # Collect train IDs\n",
        "        print(\"Collecting train IDs...\")\n",
        "        train_affected = affected[affected[amenity_type_column].str.lower() == train_type_name.lower()]\n",
        "        if len(train_affected) > 0:\n",
        "            train_ids = train_affected.groupby(property_id_col)[amenity_id_col].apply(\n",
        "                lambda x: ', '.join(x.astype(str).unique())\n",
        "            )\n",
        "            result_gdf['train_ids'] = result_gdf[property_id_col].map(\n",
        "                train_ids.to_dict()\n",
        "            ).fillna(\"\")\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"✓ Done!\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Properties affected by at least one amenity: {(result_gdf['total_amenity_count'] > 0).sum()}/{len(result_gdf)}\")\n",
        "    print(f\"Properties with trains nearby: {(result_gdf['train_ids'] != '').sum()}/{len(result_gdf)}\")\n",
        "\n",
        "    return result_gdf"
      ],
      "metadata": {
        "id": "4RTjpE1Xyf6P"
      },
      "id": "4RTjpE1Xyf6P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count amenity\n",
        "result = count_amenities(\n",
        "    property_gdf=gdf,\n",
        "    amenity_gdf=amenity_gdf,\n",
        "    radius_km=1.0,\n",
        "    train_type_name='rail station'\n",
        ")\n",
        "\n",
        "# Check what train-related types exist in your data\n",
        "print(\"Available amenity types:\")\n",
        "print(amenity_gdf['feature_type'].value_counts())"
      ],
      "metadata": {
        "id": "1CsjroOZyuS5"
      },
      "id": "1CsjroOZyuS5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc3ae42b",
      "metadata": {
        "id": "cc3ae42b"
      },
      "outputs": [],
      "source": [
        "def visualize_amenity_radius(\n",
        "    property_gdf: gpd.GeoDataFrame,\n",
        "    amenity_gdf: gpd.GeoDataFrame,\n",
        "    radius_km: float,\n",
        "    property_id: str = None,\n",
        "    map_tiles: str = 'CartoDB positron',\n",
        "    show_ridership: bool = True,\n",
        "    ridership_column: str = 'total_ridership_within_1km',\n",
        "    property_id_col: str = 'id'\n",
        ") -> folium.Map:\n",
        "    \"\"\"\n",
        "    Visualizes properties and amenities with optional ridership data.\n",
        "    Args:\n",
        "        property_gdf: GeoDataFrame with properties\n",
        "        amenity_gdf: GeoDataFrame with amenities\n",
        "        radius_km: Search radius in km\n",
        "        property_id: Specific property to highlight\n",
        "        map_tiles: Folium tile style\n",
        "        show_ridership: Whether to visualize ridership data\n",
        "        ridership_column: Column name for property ridership totals\n",
        "        property_id_col: Column name for property ID (default 'id')\n",
        "    \"\"\"\n",
        "    # Setup and CRS Alignment\n",
        "    if property_gdf.empty or 'geometry' not in property_gdf.columns:\n",
        "        print(\"Property GeoDataFrame is empty or missing geometry.\")\n",
        "        return None\n",
        "\n",
        "    if amenity_gdf.crs != property_gdf.crs:\n",
        "        amenity_gdf = amenity_gdf.to_crs(property_gdf.crs)\n",
        "\n",
        "    # Check if property_id_col exists, if not use index\n",
        "    if property_id_col not in property_gdf.columns:\n",
        "        print(f\"Warning: '{property_id_col}' column not found. Using index as ID.\")\n",
        "        property_gdf = property_gdf.copy()\n",
        "        property_gdf[property_id_col] = property_gdf.index.astype(str)\n",
        "\n",
        "    # Create Base Map\n",
        "    try:\n",
        "        center_point = property_gdf.to_crs(\"EPSG:4326\").union_all().centroid\n",
        "    except AttributeError:\n",
        "        # Fallback for older geopandas versions\n",
        "        center_point = property_gdf.to_crs(\"EPSG:4326\").unary_union.centroid\n",
        "    except Exception:\n",
        "        # Fallback to default location\n",
        "        center_point = None\n",
        "\n",
        "    if center_point:\n",
        "        m = folium.Map(location=[center_point.y, center_point.x], zoom_start=13, tiles=map_tiles)\n",
        "    else:\n",
        "        m = folium.Map(location=[3.1390, 101.6869], zoom_start=12, tiles=map_tiles)\n",
        "\n",
        "    # Define Colors and Styles\n",
        "    amenity_color_map = {\n",
        "        'rail station': '#e74c3c',  # Red for rail\n",
        "        'mall': '#9b59b6',          # Purple\n",
        "        'school': '#f39c12',        # Orange\n",
        "        'park': '#2ecc71',          # Green\n",
        "        'lake': '#3498db',          # Blue\n",
        "        'river': '#34495e',         # Dark grey\n",
        "    }\n",
        "\n",
        "    # Create ridership colormap for rail stations\n",
        "    ridership_colormap = None\n",
        "    if show_ridership and 'total_ridership' in amenity_gdf.columns:\n",
        "        rail_stations = amenity_gdf[amenity_gdf['feature_type'] == 'rail station']\n",
        "        if not rail_stations.empty and rail_stations['total_ridership'].max() > 0:\n",
        "            ridership_colormap = LinearColormap(\n",
        "                colors=['yellow', 'orange', 'red', 'darkred'],\n",
        "                vmin=rail_stations['total_ridership'].min(),\n",
        "                vmax=rail_stations['total_ridership'].max(),\n",
        "                caption='Station Ridership (Total Incoming + Outgoing)'\n",
        "            )\n",
        "\n",
        "    # Find Highlighted Property\n",
        "    highlight_property = None\n",
        "    if property_id:\n",
        "        matches = property_gdf[property_gdf[property_id_col] == property_id]\n",
        "        if not matches.empty:\n",
        "            highlight_property = matches.iloc[0]\n",
        "    if highlight_property is None and not property_gdf.empty:\n",
        "        highlight_property = property_gdf.iloc[0]\n",
        "\n",
        "    # Create Layers\n",
        "    property_layer = folium.FeatureGroup(name='Properties', show=True)\n",
        "    amenity_layer = folium.FeatureGroup(name='Amenities', show=True)\n",
        "\n",
        "    # Add Properties\n",
        "    for idx, row in property_gdf.iterrows():\n",
        "        geom = row.geometry\n",
        "        if geom is None or geom.is_empty:\n",
        "            continue\n",
        "\n",
        "        is_highlighted = highlight_property is not None and row[property_id_col] == highlight_property[property_id_col]\n",
        "\n",
        "        # Style based on ridership if available\n",
        "        if show_ridership and ridership_column in row and not pd.isna(row[ridership_column]):\n",
        "            ridership = row[ridership_column]\n",
        "            # Color intensity based on ridership\n",
        "            if ridership > 50_000_000:\n",
        "                color = '#8B0000'  # Dark red\n",
        "                weight = 6\n",
        "            elif ridership > 20_000_000:\n",
        "                color = '#FF0000'  # Red\n",
        "                weight = 5\n",
        "            elif ridership > 5_000_000:\n",
        "                color = '#FF6347'  # Tomato\n",
        "                weight = 4\n",
        "            elif ridership > 0:\n",
        "                color = '#FFA500'  # Orange\n",
        "                weight = 3\n",
        "            else:\n",
        "                color = '#6c757d'  # Grey\n",
        "                weight = 3\n",
        "        else:\n",
        "            color = 'red' if is_highlighted else '#6c757d'\n",
        "            weight = 5 if is_highlighted else 3\n",
        "\n",
        "        # Convert to lat/lon\n",
        "        g = gpd.GeoSeries.from_wkt([geom.wkt], crs=property_gdf.crs).to_crs(\"EPSG:4326\").iloc[0]\n",
        "\n",
        "        # Create popup with property info\n",
        "        popup_html = f\"<b>{row.get('road_name', 'Property')}</b><br>\"\n",
        "        popup_html += f\"ID: {row.get(property_id_col, 'N/A')}<br>\"\n",
        "        if show_ridership and ridership_column in row and not pd.isna(row[ridership_column]):\n",
        "            ridership_val = row[ridership_column]\n",
        "            popup_html += f\"<b>Total Ridership (1km): {ridership_val:,.0f}</b><br>\"\n",
        "        if 'rail_station_count' in row:\n",
        "            popup_html += f\"Rail Stations Nearby: {row['rail_station_count']}<br>\"\n",
        "\n",
        "        # Add distance columns if present\n",
        "        for col in row.index:\n",
        "            if col.startswith('dist_to_'):\n",
        "                amenity_name = col.replace('dist_to_', '').replace('_', ' ').title()\n",
        "                dist_val = row[col]\n",
        "                if not pd.isna(dist_val):\n",
        "                    popup_html += f\"{amenity_name}: {dist_val:.2f} km<br>\"\n",
        "\n",
        "        folium.PolyLine(\n",
        "            locations=[(lat, lon) for lon, lat in g.coords],\n",
        "            color=color,\n",
        "            weight=weight,\n",
        "            popup=folium.Popup(popup_html, max_width=300),\n",
        "            tooltip=row.get('road_name', 'Property')\n",
        "        ).add_to(property_layer)\n",
        "\n",
        "        # Add radius circle\n",
        "        if is_highlighted:\n",
        "            center_lat, center_lon = g.centroid.y, g.centroid.x\n",
        "            folium.Circle(\n",
        "                location=[center_lat, center_lon],\n",
        "                radius=radius_km * 1000,\n",
        "                color='red',\n",
        "                fill=True,\n",
        "                fill_opacity=0.1,\n",
        "                tooltip=f\"{radius_km} km radius\"\n",
        "            ).add_to(property_layer)\n",
        "\n",
        "    # Add Amenities\n",
        "    for idx, row in amenity_gdf.iterrows():\n",
        "        geom = row.geometry\n",
        "        if geom is None or geom.is_empty:\n",
        "            continue\n",
        "\n",
        "        feature_type = str(row.get('feature_type', 'unknown')).lower()\n",
        "\n",
        "        # Determine color\n",
        "        if feature_type == 'rail station' and show_ridership and ridership_colormap and 'total_ridership' in row:\n",
        "            ridership = row.get('total_ridership', 0)\n",
        "            if ridership > 0:\n",
        "                color = ridership_colormap(ridership)\n",
        "            else:\n",
        "                color = amenity_color_map.get(feature_type, 'gray')\n",
        "        else:\n",
        "            color = amenity_color_map.get(feature_type, 'gray')\n",
        "\n",
        "        # Create popup\n",
        "        popup_html = f\"<b>{row.get('name', 'Unnamed')}</b><br>\"\n",
        "        popup_html += f\"Type: {feature_type.title()}<br>\"\n",
        "        if 'station_id' in row:\n",
        "            popup_html += f\"Station ID: {row['station_id']}<br>\"\n",
        "        if show_ridership and 'total_ridership' in row and row['total_ridership'] > 0:\n",
        "            popup_html += f\"<b>Total Ridership: {row['total_ridership']:,.0f}</b><br>\"\n",
        "\n",
        "        # Convert to lat/lon\n",
        "        g = gpd.GeoSeries.from_wkt([geom.wkt], crs=amenity_gdf.crs).to_crs(\"EPSG:4326\").iloc[0]\n",
        "\n",
        "        if g.geom_type == 'Point':\n",
        "            # Larger markers for rail stations\n",
        "            radius = 8 if feature_type == 'rail station' else 5\n",
        "            folium.CircleMarker(\n",
        "                location=[g.y, g.x],\n",
        "                popup=folium.Popup(popup_html, max_width=250),\n",
        "                tooltip=row.get('name', feature_type.title()),\n",
        "                radius=radius,\n",
        "                color=color,\n",
        "                fill=True,\n",
        "                fill_opacity=0.8,\n",
        "                weight=2\n",
        "            ).add_to(amenity_layer)\n",
        "        elif g.geom_type == 'LineString':\n",
        "            folium.PolyLine(\n",
        "                locations=[(lat, lon) for lon, lat in g.coords],\n",
        "                popup=folium.Popup(popup_html, max_width=250),\n",
        "                color=color,\n",
        "                weight=3\n",
        "            ).add_to(amenity_layer)\n",
        "        elif g.geom_type in ['Polygon', 'MultiPolygon']:\n",
        "            folium.GeoJson(\n",
        "                g,\n",
        "                popup=folium.Popup(popup_html, max_width=250),\n",
        "                style_function=lambda x, color=color: {\n",
        "                    'fillColor': color,\n",
        "                    'color': color,\n",
        "                    'weight': 2,\n",
        "                    'fillOpacity': 0.4,\n",
        "                }\n",
        "            ).add_to(amenity_layer)\n",
        "\n",
        "    # Add Layers to Map\n",
        "    property_layer.add_to(m)\n",
        "    amenity_layer.add_to(m)\n",
        "\n",
        "    # Create Legend\n",
        "    legend_html = f'''\n",
        "    <div style=\"position: fixed; bottom: 50px; left: 50px; width: 250px;\n",
        "                border:2px solid grey; z-index:9999; font-size:13px;\n",
        "                background-color:white; padding: 10px; opacity: 0.9;\">\n",
        "        <div style=\"font-weight: bold; margin-bottom: 8px; font-size: 15px;\">Legend</div>\n",
        "        <div><i style=\"background:red; width:15px; height:3px; display:inline-block; margin-right:5px;\"></i>Highlighted Property</div>\n",
        "        <div><i style=\"border: 2px solid red; border-radius:50%; width:12px; height:12px; display:inline-block; margin-right:5px;\"></i>Search Radius ({radius_km} km)</div>\n",
        "        <hr style=\"margin: 8px 0;\">\n",
        "        <div style=\"font-weight: bold; margin-bottom: 5px;\">Amenity Types</div>\n",
        "    '''\n",
        "\n",
        "    for f_type, f_color in amenity_color_map.items():\n",
        "        legend_html += f'<div><i style=\"background:{f_color}; width:12px; height:12px; border-radius:50%; display:inline-block; margin-right:5px;\"></i>{f_type.title()}</div>'\n",
        "\n",
        "    if show_ridership and ridership_colormap:\n",
        "        legend_html += '<hr style=\"margin: 8px 0;\">'\n",
        "        legend_html += '<div style=\"font-weight: bold; margin-bottom: 5px;\">Rail Station Ridership</div>'\n",
        "        legend_html += '<div style=\"font-size: 11px;\">Darker = Higher Ridership</div>'\n",
        "\n",
        "    legend_html += '</div>'\n",
        "\n",
        "    m.get_root().html.add_child(folium.Element(legend_html))\n",
        "\n",
        "    # Add ridership colormap if available\n",
        "    if ridership_colormap:\n",
        "        ridership_colormap.add_to(m)\n",
        "\n",
        "    # Add Controls\n",
        "    folium.LayerControl(collapsed=False).add_to(m)\n",
        "\n",
        "    return m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-IXHcIWufHcN",
      "metadata": {
        "id": "-IXHcIWufHcN"
      },
      "outputs": [],
      "source": [
        "search_radius_km = 1\n",
        "map = visualize_amenity_radius(\n",
        "        result,\n",
        "        amenity_gdf,\n",
        "        search_radius_km,\n",
        "        property_id_col='records_id'\n",
        "    )\n",
        "map"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_ridership_by_station_id_with_date(\n",
        "    property_gdf,\n",
        "    amenity_gdf,\n",
        "    ridership_df,\n",
        "    radius_km=1.0,\n",
        "    property_id_col='index',\n",
        "    property_date_col='date',\n",
        "    ridership_date_col='date',\n",
        "    amenity_id_col='osm_id'\n",
        "):\n",
        "    \"\"\"\n",
        "    Join ridership to properties matching by station_id AND date.\n",
        "    Returns separate columns for incoming and outgoing ridership.\n",
        "    \"\"\"\n",
        "\n",
        "    # Prepare ridership data\n",
        "    print(\"Preparing ridership data...\")\n",
        "    ridership_clean = ridership_df[ridership_df['station_id'] != 'A0'].copy()\n",
        "    ridership_clean[ridership_date_col] = pd.to_datetime(ridership_clean[ridership_date_col])\n",
        "\n",
        "    station_ridership = ridership_clean.groupby(['station_id', ridership_date_col]).agg({\n",
        "        'incoming': 'sum',\n",
        "        'outgoing': 'sum'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Keep incoming and outgoing separate\n",
        "    station_ridership['total_ridership'] = (\n",
        "        station_ridership['incoming'] + station_ridership['outgoing']\n",
        "    )\n",
        "\n",
        "    print(f\"✓ Prepared ridership for {len(station_ridership['station_id'].unique())} stations\")\n",
        "    print(f\"  Date range: {station_ridership[ridership_date_col].min()} to {station_ridership[ridership_date_col].max()}\")\n",
        "\n",
        "    # Join ridership with rail stations\n",
        "    print(\"\\nJoining ridership with station locations...\")\n",
        "    rail_stations = amenity_gdf[amenity_gdf['feature_type'] == 'rail station'].copy()\n",
        "\n",
        "    # Rename date column to avoid conflicts in spatial join\n",
        "    station_ridership = station_ridership.rename(columns={ridership_date_col: 'ridership_date'})\n",
        "\n",
        "    # Merge with ALL ridership columns (incoming, outgoing, total)\n",
        "    stations_with_ridership = rail_stations.merge(\n",
        "        station_ridership[['station_id', 'ridership_date', 'incoming', 'outgoing', 'total_ridership']],\n",
        "        on='station_id',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    print(f\"✓ Created {len(stations_with_ridership):,} station-date combinations\")\n",
        "\n",
        "    # Project and buffer\n",
        "    print(f\"Buffering stations ({radius_km}km)...\")\n",
        "    stations_projected = stations_with_ridership.to_crs('EPSG:32648')\n",
        "    property_projected = property_gdf.copy()\n",
        "    property_projected = property_projected.to_crs('EPSG:32648')\n",
        "\n",
        "    # Rename property date column to avoid conflicts\n",
        "    property_projected = property_projected.rename(columns={property_date_col: 'property_date'})\n",
        "\n",
        "    radius_meters = radius_km * 1000\n",
        "    stations_buffered = stations_projected.copy()\n",
        "    stations_buffered['geometry'] = stations_buffered.geometry.buffer(radius_meters)\n",
        "\n",
        "    # Spatial join\n",
        "    print(\"Finding affected properties...\")\n",
        "    affected = gpd.sjoin(\n",
        "        stations_buffered[[amenity_id_col, 'station_id', 'ridership_date', 'incoming', 'outgoing', 'total_ridership', 'geometry']],\n",
        "        property_projected[[property_id_col, 'property_date', 'geometry']],\n",
        "        how='inner',\n",
        "        predicate='intersects'\n",
        "    )\n",
        "\n",
        "    print(f\"✓ Found {len(affected):,} station-property-date combinations\")\n",
        "\n",
        "    # Filter to matching dates\n",
        "    print(f\"Matching dates between properties and ridership...\")\n",
        "\n",
        "    # Ensure both are datetime\n",
        "    affected['property_date'] = pd.to_datetime(affected['property_date'])\n",
        "    affected['ridership_date'] = pd.to_datetime(affected['ridership_date'])\n",
        "\n",
        "    # Keep only matching dates\n",
        "    affected_matched = affected[affected['property_date'] == affected['ridership_date']].copy()\n",
        "\n",
        "    print(f\"✓ Matched {len(affected_matched):,} station-property pairs with same dates\")\n",
        "\n",
        "    if len(affected_matched) == 0:\n",
        "        print(\"\\n⚠ Warning: No date matches found!\")\n",
        "        print(f\"\\nProperty dates sample:\")\n",
        "        print(property_gdf[property_date_col].dt.date.unique()[:10])\n",
        "        print(f\"\\nRidership dates sample:\")\n",
        "        print(ridership_clean[ridership_date_col].dt.date.unique()[:10])\n",
        "\n",
        "        # Check overlap\n",
        "        prop_dates = set(pd.to_datetime(property_gdf[property_date_col]).dt.date)\n",
        "        rider_dates = set(pd.to_datetime(ridership_clean[ridership_date_col]).dt.date)\n",
        "        overlap = prop_dates.intersection(rider_dates)\n",
        "        print(f\"\\nOverlapping dates: {len(overlap)}\")\n",
        "        if len(overlap) > 0:\n",
        "            print(f\"Examples: {list(overlap)[:5]}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    # Step 6: Sum ridership per property (SEPARATE incoming and outgoing)\n",
        "    print(\"Summing ridership for each property...\")\n",
        "    ridership_sum = affected_matched.groupby(property_id_col).agg({\n",
        "        'incoming': 'sum',\n",
        "        'outgoing': 'sum',\n",
        "        'total_ridership': 'sum'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Rename columns to be descriptive\n",
        "    ridership_sum.columns = [\n",
        "        property_id_col,\n",
        "        'incoming_ridership_within_1km',\n",
        "        'outgoing_ridership_within_1km',\n",
        "        'total_ridership_within_1km'\n",
        "    ]\n",
        "\n",
        "    # Step 7: Join back to original properties\n",
        "    result_gdf = property_gdf.copy()\n",
        "    result_gdf = result_gdf.merge(ridership_sum, on=property_id_col, how='left')\n",
        "\n",
        "    # Fill NaN with 0\n",
        "    result_gdf['incoming_ridership_within_1km'] = result_gdf['incoming_ridership_within_1km'].fillna(0)\n",
        "    result_gdf['outgoing_ridership_within_1km'] = result_gdf['outgoing_ridership_within_1km'].fillna(0)\n",
        "    result_gdf['total_ridership_within_1km'] = result_gdf['total_ridership_within_1km'].fillna(0)\n",
        "\n",
        "    # Summary\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"✓ COMPLETE!\")\n",
        "    print(f\"{'='*60}\")\n",
        "    has_ridership = (result_gdf['total_ridership_within_1km'] > 0).sum()\n",
        "    print(f\"Properties with ridership: {has_ridership:,}/{len(result_gdf):,} ({has_ridership/len(result_gdf)*100:.1f}%)\")\n",
        "    print(f\"\\nRidership Statistics:\")\n",
        "    print(f\"  Incoming - Avg: {result_gdf['incoming_ridership_within_1km'].mean():,.0f}, Max: {result_gdf['incoming_ridership_within_1km'].max():,.0f}\")\n",
        "    print(f\"  Outgoing - Avg: {result_gdf['outgoing_ridership_within_1km'].mean():,.0f}, Max: {result_gdf['outgoing_ridership_within_1km'].max():,.0f}\")\n",
        "    print(f\"  Total    - Avg: {result_gdf['total_ridership_within_1km'].mean():,.0f}, Max: {result_gdf['total_ridership_within_1km'].max():,.0f}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    return result_gdf"
      ],
      "metadata": {
        "id": "VgQCCqyg2hsT"
      },
      "id": "VgQCCqyg2hsT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate ridership\n",
        "result_with_ridership = add_ridership_by_station_id_with_date(\n",
        "    property_gdf=result,\n",
        "    amenity_gdf=amenity_gdf,\n",
        "    ridership_df=ridership,\n",
        "    radius_km=1.0,\n",
        "    property_date_col='date',\n",
        "    ridership_date_col='date'\n",
        ")"
      ],
      "metadata": {
        "id": "DACZjs4b89T8"
      },
      "id": "DACZjs4b89T8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ridership check"
      ],
      "metadata": {
        "id": "W9mE1Bw5hqQR"
      },
      "id": "W9mE1Bw5hqQR"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the IDs to exclude\n",
        "## Excluding lines from Shah Alam and most in PYL and 1 KJ37\n",
        "exclude_ids = [\n",
        "    10211729882, 7932664086, 10211729880, 10211729878, 10211729876,\n",
        "    10211729874, 10211729872, 10211729870, 7725136738, 10211729860,\n",
        "    10211729858, 10211729856, 10211729854, 10211729852, 10132470682,\n",
        "    10211729850, 7724226582, 10211729846, 10211729845, 10211729843,\n",
        "    10211729841, 10211729839, 9755011838, 9755011836, 9755011840,\n",
        "    9833287690, 9833287692, 9833371330, 9849953874, 9849953872,\n",
        "    7723739344, 7723739292, 10605496579, 10605496582, 10605496584,\n",
        "    10605496586, 10605496587, 10605496590,10605496592,10605496594,\n",
        "    10605496596, 10605496598, 10605496600, 10605496604,5702102258, 10605504105,\n",
        "    5702102312, 5702102333,10605504107, 10591412710, 10605504109, 10605504111, 7077855554, 7723703874\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# Convert to string for comparison since train_ids appears to be strings\n",
        "exclude_ids_str = [str(id) for id in exclude_ids]\n",
        "\n",
        "# Filter using a custom function to check if ANY excluded ID is in train_ids\n",
        "def contains_excluded_id(train_ids_str, exclude_list):\n",
        "    \"\"\"Check if train_ids string contains any excluded IDs\"\"\"\n",
        "    if train_ids_str == \"\":\n",
        "        return False\n",
        "    # Split the train_ids by comma\n",
        "    ids_in_cell = [id.strip() for id in train_ids_str.split(',')]\n",
        "    # Check if any ID is in exclude list\n",
        "    return any(id in exclude_list for id in ids_in_cell)\n",
        "\n",
        "# Apply filter\n",
        "mask = (\n",
        "    (result_with_ridership['train_ids'] != \"\") &\n",
        "    (result_with_ridership['total_ridership_within_1km'] == 0) &\n",
        "    (~result_with_ridership['train_ids'].apply(lambda x: contains_excluded_id(x, exclude_ids_str)))\n",
        ")\n",
        "\n",
        "filtered_result = result_with_ridership[mask][['train_ids']].value_counts()\n",
        "print(filtered_result)"
      ],
      "metadata": {
        "id": "FHIMlVZ0gdV5"
      },
      "id": "FHIMlVZ0gdV5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dist"
      ],
      "metadata": {
        "id": "bPpl3oy4LeOP"
      },
      "id": "bPpl3oy4LeOP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6viHafaAvYAY",
      "metadata": {
        "id": "6viHafaAvYAY"
      },
      "outputs": [],
      "source": [
        "def download_road_graph(place_osm_ids: list, network_type: str = 'drive') -> nx.MultiDiGraph:\n",
        "    \"\"\"\n",
        "    Downloads the road network for a specified list of OSM area IDs and merges them.\n",
        "    \"\"\"\n",
        "    print(f\"Downloading road networks for {len(place_osm_ids)} areas...\")\n",
        "    try:\n",
        "        # Download boundary polygons INDIVIDUALLY for each OSM ID\n",
        "        boundaries_list = []\n",
        "        for osm_id in place_osm_ids:\n",
        "            query_string = f\"R{osm_id}\"\n",
        "            boundary_gdf = ox.geocode_to_gdf(query_string, by_osmid=True)\n",
        "            boundaries_list.append(boundary_gdf)\n",
        "\n",
        "        # Combine all boundaries into one GeoDataFrame\n",
        "        boundaries_gdf = pd.concat(boundaries_list, ignore_index=True)\n",
        "        print(f\"✅ Retrieved {len(boundaries_gdf)} boundary polygons\")\n",
        "\n",
        "        # Download the graph for each polygon individually\n",
        "        graphs = []\n",
        "        for idx, row in boundaries_gdf.iterrows():\n",
        "            polygon = row['geometry']\n",
        "            area_name = row['display_name'].split(',')[0]\n",
        "            print(f\" -> Downloading '{network_type}' network for {area_name}...\")\n",
        "            G_area = ox.graph_from_polygon(polygon, network_type=network_type)\n",
        "            graphs.append(G_area)\n",
        "\n",
        "        # Merge all the individual graphs into one\n",
        "        print(\"\\nComposing individual graphs into a single network...\")\n",
        "        G_combined = nx.compose_all(graphs)\n",
        "        print(f\"✅ Successfully downloaded and combined {len(graphs)} graphs.\")\n",
        "\n",
        "        return G_combined\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during the graph download process: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_representative_point(geom):\n",
        "    \"\"\"\n",
        "    Helper function to get a representative (latitude, longitude) point from various geometry types.\n",
        "    For lines and polygons, it returns the centroid.\n",
        "    \"\"\"\n",
        "    if geom is None or geom.is_empty:\n",
        "        return None, None\n",
        "    if isinstance(geom, Point):\n",
        "        return geom.y, geom.x\n",
        "    # For lines and polygons, the centroid is a good representative point\n",
        "    centroid = geom.centroid\n",
        "    return centroid.y, centroid.x\n",
        "\n",
        "def calculate_travel_distances_fastest(\n",
        "    property_gdf: gpd.GeoDataFrame,\n",
        "    amenity_gdf: gpd.GeoDataFrame,\n",
        "    G: nx.MultiDiGraph,\n",
        "    max_distance_km: float = 3000.0\n",
        ") -> gpd.GeoDataFrame:\n",
        "    \"\"\"\n",
        "    FASTEST approach: Calculate from each property to nearest amenities\n",
        "    \"\"\"\n",
        "    print(\"Ultra-optimized calculation started...\")\n",
        "\n",
        "    # CRS alignment\n",
        "    graph_crs = G.graph['crs']\n",
        "    if property_gdf.crs != graph_crs:\n",
        "        property_gdf = property_gdf.to_crs(graph_crs)\n",
        "    if amenity_gdf.crs != graph_crs:\n",
        "        amenity_gdf = amenity_gdf.to_crs(graph_crs)\n",
        "\n",
        "    result_gdf = property_gdf.copy()\n",
        "    cutoff_meters = max_distance_km * 1000\n",
        "\n",
        "    # Map to nodes\n",
        "    print(\"Mapping to network...\")\n",
        "    prop_lats = result_gdf.geometry.apply(lambda g: get_representative_point(g)[0])\n",
        "    prop_lons = result_gdf.geometry.apply(lambda g: get_representative_point(g)[1])\n",
        "    property_nodes = pd.Series(\n",
        "        ox.nearest_nodes(G, prop_lons.to_list(), prop_lats.to_list()),\n",
        "        index=result_gdf.index\n",
        "    )\n",
        "\n",
        "    amenity_lats = amenity_gdf.geometry.apply(lambda g: get_representative_point(g)[0])\n",
        "    amenity_lons = amenity_gdf.geometry.apply(lambda g: get_representative_point(g)[1])\n",
        "    amenity_nodes = pd.Series(\n",
        "        ox.nearest_nodes(G, amenity_lons.to_list(), amenity_lats.to_list()),\n",
        "        index=amenity_gdf.index\n",
        "    )\n",
        "\n",
        "    # Create amenity type lookup\n",
        "    amenity_type_nodes = {}\n",
        "    for amenity_type in amenity_gdf['feature_type'].dropna().unique():\n",
        "        amenity_indices = amenity_gdf[amenity_gdf['feature_type'] == amenity_type].index\n",
        "        amenity_type_nodes[amenity_type] = set(amenity_nodes.loc[amenity_indices])\n",
        "\n",
        "        # Initialize column\n",
        "        col_name = f\"dist_to_{str(amenity_type).lower().replace(' ', '_')}\"\n",
        "        result_gdf[col_name] = np.nan\n",
        "\n",
        "    # Process each property\n",
        "    print(\"Calculating distances from each property...\")\n",
        "    for idx, prop_node in tqdm(property_nodes.items(), total=len(property_nodes)):\n",
        "        try:\n",
        "            # Calculate distances from this property to nearby nodes\n",
        "            distances = nx.single_source_dijkstra_path_length(\n",
        "                G, prop_node, cutoff=cutoff_meters, weight='length'\n",
        "            )\n",
        "\n",
        "            # For each amenity type, find minimum distance\n",
        "            for amenity_type, target_nodes in amenity_type_nodes.items():\n",
        "                col_name = f\"dist_to_{str(amenity_type).lower().replace(' ', '_')}\"\n",
        "\n",
        "                # Find minimum distance to any amenity of this type\n",
        "                min_dist = min(\n",
        "                    (distances[node] for node in target_nodes if node in distances),\n",
        "                    default=np.nan\n",
        "                )\n",
        "\n",
        "                result_gdf.at[idx, col_name] = min_dist / 1000  # Convert to km\n",
        "\n",
        "        except (nx.NetworkXNoPath, nx.NodeNotFound):\n",
        "            continue\n",
        "\n",
        "    print(\"Calculation complete.\")\n",
        "    return result_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ko3qoEJbyZK",
      "metadata": {
        "id": "3ko3qoEJbyZK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Define the OSM IDs for the combined road network area.\n",
        "DISTRICT_OSM_IDS = [2939672, # Kuala Lumpur\n",
        "                     4443881, #Putrajaya\n",
        "                     12438352, #GOMBAK\n",
        "                     12438351, #HULU LANGAT\n",
        "                     12391135, #KLANG\n",
        "                     10714199, #HULU SELANGOR\n",
        "                     10743362, #KUALA LANGAT\n",
        "                     10714137, #KUALA SELANGOR\n",
        "                     12391134, #PETALING\n",
        "                     10714136, #SABAK BERNAM\n",
        "                     10743315 #SEPANG\n",
        "                     ]\n",
        "\n",
        "# Define the save path\n",
        "graph_filepath = '/content/drive/MyDrive/Colab/Capstone 1/klang_valley_road_network.graphml'\n",
        "\n",
        "# Check if graph already exists, if not download and save it\n",
        "if os.path.exists(graph_filepath):\n",
        "    print(\"Loading existing road network graph...\")\n",
        "    road_network_graph = ox.load_graphml(graph_filepath)\n",
        "    print(\"✅ Graph loaded from file!\")\n",
        "else:\n",
        "    print(\"Downloading road network graph...\")\n",
        "    road_network_graph = download_road_graph(\n",
        "        place_osm_ids=DISTRICT_OSM_IDS,\n",
        "        network_type='drive'\n",
        "    )\n",
        "\n",
        "    # Save the graph\n",
        "    print(\"Saving road network graph...\")\n",
        "    ox.save_graphml(road_network_graph, filepath=graph_filepath)\n",
        "    print(f\"✅ Graph saved to {graph_filepath}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the walking save path\n",
        "walk_graph_filepath = '/content/drive/MyDrive/Colab/Capstone 1/klang_valley_walking_network.graphml'\n",
        "\n",
        "# Check if graph already exists, if not download and save it\n",
        "if os.path.exists(walk_graph_filepath):\n",
        "    print(\"Loading existing walking network graph...\")\n",
        "    roadwalk_network_graph = ox.load_graphml(walk_graph_filepath)\n",
        "    print(\"✅ Graph loaded from file!\")\n",
        "else:\n",
        "    print(\"Downloading walking network graph...\")\n",
        "    roadwalk_network_graph = download_road_graph(\n",
        "        place_osm_ids=DISTRICT_OSM_IDS,\n",
        "        network_type='walk'\n",
        "    )\n",
        "\n",
        "    # Save the graph\n",
        "    print(\"Saving walking network graph...\")\n",
        "    ox.save_graphml(roadwalk_network_graph, filepath=walk_graph_filepath)\n",
        "    print(f\"✅ Graph saved to {walk_graph_filepath}\")\n"
      ],
      "metadata": {
        "id": "rsNp990Q6AZC"
      },
      "id": "rsNp990Q6AZC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicate properties by records_id (not geometry)\n",
        "property_gdf_unique = result_with_ridership.drop_duplicates(subset=['way_id'])\n",
        "\n",
        "# Calculate WALKING distances\n",
        "df_walk_result = calculate_travel_distances_fastest(\n",
        "    property_gdf=property_gdf_unique,\n",
        "    amenity_gdf=amenity_gdf,\n",
        "    G=roadwalk_network_graph,\n",
        "    max_distance_km=3000.0\n",
        ")\n",
        "\n",
        "# Rename walking distance columns\n",
        "distance_cols = [col for col in df_walk_result.columns if col.startswith('dist_to_')]\n",
        "rename_dict = {col: f'walk_{col}' for col in distance_cols}\n",
        "df_walk_result.rename(columns=rename_dict, inplace=True)\n",
        "\n",
        "# Calculate DRIVING distances\n",
        "df_drive_result = calculate_travel_distances_fastest(\n",
        "    property_gdf=property_gdf_unique,\n",
        "    amenity_gdf=amenity_gdf,\n",
        "    G=road_network_graph,\n",
        "    max_distance_km=3000.0\n",
        ")\n",
        "\n",
        "# Merge walking + driving on records_id\n",
        "walk_cols = ['way_id'] + [col for col in df_walk_result.columns if col.startswith('walk_')]\n",
        "df_combined = df_walk_result[walk_cols].merge(\n",
        "    df_drive_result[['way_id'] + [col for col in df_drive_result.columns if col.startswith('dist_to_')]],\n",
        "    on='way_id',\n",
        "    how='outer'  # Use outer to keep all properties\n",
        ")\n",
        "\n",
        "# Join back to original\n",
        "optimized_distances_df = result_with_ridership.merge(\n",
        "    df_combined,\n",
        "    on='way_id',\n",
        "    how='left'\n",
        ")"
      ],
      "metadata": {
        "id": "Rmx7gayn6YP0"
      },
      "id": "Rmx7gayn6YP0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select distance columns\n",
        "dist_cols = [col for col in optimized_distances_df.columns if 'dist_to_' in col]\n",
        "\n",
        "# Check data quality\n",
        "print(f\"\\n📊 Distance columns: {len(dist_cols)}\")\n",
        "print(f\"❌ Missing values:\\n{optimized_distances_df[dist_cols].isna().sum()}\")\n",
        "\n",
        "# Clean up dataframe\n",
        "optimized_distances_df = (\n",
        "    optimized_distances_df\n",
        "    .filter(regex='^(?!.*_y$)')  # Remove _y columns\n",
        "    .rename(columns=lambda x: x.replace('_x', ''))  # Rename _x suffixes\n",
        "    .round({col: 4 for col in dist_cols})  # Round distances to 4 decimals\n",
        ")\n",
        "\n",
        "# Preview results\n",
        "print(f\"\\n✅ Final shape: {optimized_distances_df.shape}\")\n",
        "optimized_distances_df.head()"
      ],
      "metadata": {
        "id": "MlDD02IGhzj5"
      },
      "id": "MlDD02IGhzj5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "228d7846",
      "metadata": {
        "id": "228d7846"
      },
      "outputs": [],
      "source": [
        "# Export dataset for further analysis\n",
        "#optimized_distances_df.to_excel(\"/content/drive/MyDrive/Colab/Capstone 1/optimized_distances_df.xlsx\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}