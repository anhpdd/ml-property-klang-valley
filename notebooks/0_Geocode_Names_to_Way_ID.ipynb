{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUX+ZfLGUlpb5ZV4nCHcRw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhpdd/ml-property-valuation-klang-valley/blob/main/notebooks/0_Geocode_Names_to_Way_ID.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "CS69s2WitMlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q osmnx geopy tqdm\n",
        "\n",
        "# Core\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "import osmnx as ox\n",
        "import networkx as nx\n",
        "\n",
        "# Geospatial\n",
        "from shapely.geometry import Point, LineString, Polygon, MultiPolygon\n",
        "from shapely.ops import unary_union, polygonize\n",
        "from shapely import wkt\n",
        "from geopy.distance import great_circle\n",
        "\n",
        "# API\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.extra.rate_limiter import RateLimiter\n",
        "\n",
        "# Utilities\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "from typing import List, Dict, Any, Optional\n",
        "from tqdm.auto import tqdm\n",
        "from IPython.display import display\n",
        "\n",
        "# Visualization\n",
        "import folium\n",
        "\n",
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0Ix2MAxguZlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all values from the worksheet and convert them into a pandas DataFrame\n",
        "file_path = r\"file_path\\something.csv\" # Insert your file path here\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Validate data\n",
        "if df.empty or 'Road Name' not in df.columns:\n",
        "    raise ValueError(\"âŒ DataFrame is empty or 'Road Name' column missing\")\n",
        "else:\n",
        "  df.info()\n",
        "  print(f\"âœ… Loaded {len(df):,} rows\")"
      ],
      "metadata": {
        "id": "G3rOR65StLrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GEOCODE ROAD NAMES TO OSM WAY IDs"
      ],
      "metadata": {
        "id": "1JHgo9jetXUk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byohtd0wtGWe"
      },
      "outputs": [],
      "source": [
        "# Setup geocoder\n",
        "unique_roads = df['Road Name'].dropna().unique()\n",
        "print(f\"\\nðŸ“ Geocoding {len(unique_roads):,} unique road names...\")\n",
        "\n",
        "geolocator = Nominatim(user_agent=\"selangor_road_wayid_extractor_v2\")\n",
        "geocode = RateLimiter(\n",
        "    geolocator.geocode,\n",
        "    min_delay_seconds=1.1,\n",
        "    error_wait_seconds=10.0,\n",
        "    max_retries=2\n",
        ")\n",
        "\n",
        "# Selangor bounding box (to prevent global results)\n",
        "SELANGOR_BBOX = [[2.553, 100.841], [3.716, 102.39]]\n",
        "\n",
        "# Geocode all unique roads\n",
        "road_to_way_id = {}\n",
        "road_to_node_info = {}\n",
        "\n",
        "for road_name in tqdm(unique_roads, desc=\"Geocoding\"):\n",
        "    # Initialize\n",
        "    road_to_way_id[road_name] = np.nan\n",
        "    road_to_node_info[road_name] = (np.nan, np.nan)\n",
        "\n",
        "    try:\n",
        "        # Query Nominatim\n",
        "        locations = geocode(\n",
        "            f\"{road_name}, Malaysia\",\n",
        "            timeout=10,\n",
        "            viewbox=SELANGOR_BBOX,\n",
        "            bounded=True,\n",
        "            exactly_one=False,\n",
        "            limit=5\n",
        "        )\n",
        "\n",
        "        if not locations:\n",
        "            continue\n",
        "\n",
        "        # Prefer 'way' type results\n",
        "        for loc in locations:\n",
        "            if loc.raw.get('osm_type') == 'way':\n",
        "                road_to_way_id[road_name] = loc.raw.get('osm_id')\n",
        "                break\n",
        "        else:\n",
        "            # Fallback to 'node' if no way found\n",
        "            top_result = locations[0]\n",
        "            if top_result.raw.get('osm_type') == 'node':\n",
        "                road_to_node_info[road_name] = (\n",
        "                    top_result.raw.get('osm_id'),\n",
        "                    top_result.address\n",
        "                )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  Error geocoding '{road_name}': {e}\")\n",
        "\n",
        "print(\"âœ… Geocoding complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MAP RESULTS BACK TO DATAFRAME"
      ],
      "metadata": {
        "id": "fLmF5YsDt-SE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add way_id column\n",
        "df['way_id'] = df['Road Name'].map(road_to_way_id)\n",
        "\n",
        "# Add node info columns\n",
        "node_info = df['Road Name'].map(road_to_node_info)\n",
        "df['found_node_id'] = node_info.str[0]\n",
        "df['found_node_name'] = node_info.str[1]\n",
        "\n",
        "# Save unique results\n",
        "unique_results = pd.DataFrame({\n",
        "    'way_id': road_to_way_id,\n",
        "    'found_node_id': [road_to_node_info[r][0] for r in road_to_way_id.keys()],\n",
        "    'found_node_name': [road_to_node_info[r][1] for r in road_to_way_id.keys()]\n",
        "}, index=pd.Index(road_to_way_id.keys(), name='Road_Name'))\n",
        "\n",
        "#unique_results.to_csv('geocoded_unique_road_names.csv')\n",
        "#print(\"âœ… Saved unique results to 'geocoded_unique_road_names.csv'\")\n",
        "\n",
        "\n",
        "# SUMMARY STATISTICS\n",
        "total = len(df)\n",
        "way_found = df['way_id'].notna().sum()\n",
        "node_found = df['found_node_id'].notna().sum()\n",
        "not_found = total - way_found - node_found\n",
        "\n",
        "print(f\"\\nðŸ“Š Geocoding Summary:\")\n",
        "print(f\"   â€¢ Way IDs found: {way_found:,} ({way_found/total:.1%})\")\n",
        "print(f\"   â€¢ Node fallbacks: {node_found:,} ({node_found/total:.1%})\")\n",
        "print(f\"   â€¢ Not found: {not_found:,} ({not_found/total:.1%})\")"
      ],
      "metadata": {
        "id": "XMqpHAQit9cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 2"
      ],
      "metadata": {
        "id": "xPQPvKz8uvdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OSM_API_BASE_URL = \"https://api.openstreetmap.org/api/0.6\"\n",
        "\n",
        "# Define your headers once\n",
        "HEADERS = {\n",
        "    'User-Agent': 'MyDataProject/1.0 (https://example.com; myemail@example.com)'\n",
        "}\n",
        "\n",
        "def fetch_osm_data(url: str, timeout: int = 25) -> ET.Element | None:\n",
        "    \"\"\"\n",
        "    Fetches data from the OSM API and parses it as XML.\n",
        "    Includes a required User-Agent header.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Add the headers to your request\n",
        "        response = requests.get(url, timeout=timeout, headers=HEADERS)\n",
        "        response.raise_for_status()\n",
        "        return ET.fromstring(response.content)\n",
        "\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        # Your excellent 404 handling\n",
        "        if e.response is not None and e.response.status_code == 404:\n",
        "            return None\n",
        "        print(f\"HTTP Error for {url}: {e}\")\n",
        "        return None # Explicitly return None on other HTTP errors\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Request failed for {url}: {e}\")\n",
        "        return None # Explicitly return None on failure\n",
        "\n",
        "    except ET.ParseError as e:\n",
        "        print(f\"XML parsing failed for {url}. Error: {e}\")\n",
        "        return None # Explicitly return None on failure\n",
        "\n",
        "\n",
        "def extract_line_geometry(input_df: pd.DataFrame, name_column: str, id_column: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fetches OSM data for Way IDs and adds road geometry to the input DataFrame.\n",
        "\n",
        "    Args:\n",
        "        input_df: DataFrame with Way IDs\n",
        "        name_column: Column containing road/location name\n",
        "        id_column: Column containing OSM Way ID\n",
        "\n",
        "    Returns:\n",
        "        Original DataFrame with added 'geometry' column (NaN for failed extractions)\n",
        "    \"\"\"\n",
        "    print(f\"ðŸ“ Extracting geometry for {len(input_df):,} Way IDs...\")\n",
        "\n",
        "    # Create a dictionary to store way_id -> geometry mapping\n",
        "    way_id_to_geometry = {}\n",
        "\n",
        "    # Process each unique way_id\n",
        "    unique_way_ids = input_df[id_column].dropna().unique()\n",
        "\n",
        "    for way_id in tqdm(unique_way_ids, desc=\"Fetching geometries\"):\n",
        "        way_id_str = str(int(way_id))\n",
        "\n",
        "        # Fetch OSM data\n",
        "        url = f\"{OSM_API_BASE_URL}/way/{way_id_str}/full\"\n",
        "        root = fetch_osm_data(url)\n",
        "\n",
        "        if not root:\n",
        "            continue\n",
        "\n",
        "        # Find the way element\n",
        "        road_way_elem = root.find(f\".//way[@id='{way_id_str}']\")\n",
        "        if not road_way_elem:\n",
        "            continue\n",
        "\n",
        "        # Cache node coordinates\n",
        "        node_coords_cache = {}\n",
        "        for node_elem in root.findall(\".//node\"):\n",
        "            try:\n",
        "                node_id = node_elem.get('id')\n",
        "                lat = float(node_elem.get('lat'))\n",
        "                lon = float(node_elem.get('lon'))\n",
        "                node_coords_cache[node_id] = (lon, lat)\n",
        "            except (TypeError, ValueError):\n",
        "                continue\n",
        "\n",
        "        # Build coordinate list\n",
        "        road_coords = []\n",
        "        for nd_ref_elem in road_way_elem.findall('nd'):\n",
        "            node_ref = nd_ref_elem.get('ref')\n",
        "            if node_ref in node_coords_cache:\n",
        "                road_coords.append(node_coords_cache[node_ref])\n",
        "\n",
        "        # Create LineString if valid\n",
        "        if len(road_coords) >= 2:\n",
        "            way_id_to_geometry[way_id] = LineString(road_coords)\n",
        "\n",
        "    # Map geometries back to original DataFrame\n",
        "    output_df = input_df.copy()\n",
        "    output_df['geometry'] = output_df[id_column].map(way_id_to_geometry)\n",
        "\n",
        "    # Summary\n",
        "    success_count = output_df['geometry'].notna().sum()\n",
        "    print(f\"âœ… Extracted geometry for {success_count:,}/{len(input_df):,} rows ({success_count/len(input_df)*100:.1f}%)\")\n",
        "\n",
        "    return output_df\n",
        "\n",
        "def generate_geometry_summary(\n",
        "    result_gdf: Optional[gpd.GeoDataFrame],\n",
        "    original_df: pd.DataFrame,\n",
        "    name_column: str\n",
        ") -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Analyzes the result of the geometry extraction and prints a summary.\n",
        "\n",
        "    Compares the final GeoDataFrame against the initial DataFrame to determine\n",
        "    success and failure rates for creating road geometries.\n",
        "\n",
        "    Args:\n",
        "        result_gdf (Optional[gpd.GeoDataFrame]): The GeoDataFrame returned by the\n",
        "                                                 extract_line_geometry function.\n",
        "                                                 Can be None if the process failed entirely.\n",
        "        original_df (pd.DataFrame): The original DataFrame that was passed to the\n",
        "                                    geometry extraction function.\n",
        "        name_column (str): The name of the column containing the unique road/location names.\n",
        "\n",
        "    Returns:\n",
        "        Optional[pd.DataFrame]: A DataFrame containing the rows of the original data\n",
        "                                that failed to produce a valid geometry, or None if all\n",
        "                                were successful.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Geometry Extraction Summary ---\")\n",
        "\n",
        "    if original_df.empty:\n",
        "        print(\"Original DataFrame is empty. No roads to process.\")\n",
        "        return None\n",
        "\n",
        "    total_unique_roads = original_df[name_column].nunique()\n",
        "    print(f\"Total unique roads attempted: {total_unique_roads}\")\n",
        "\n",
        "    if result_gdf is None or result_gdf.empty:\n",
        "        print(\"No geometries were successfully created.\")\n",
        "        print(f\"Success rate: 0.00%\")\n",
        "        print(f\"Failure rate: 100.00%\")\n",
        "        return original_df # All failed records\n",
        "\n",
        "    successful_count = len(result_gdf)\n",
        "    failed_count = total_unique_roads - successful_count\n",
        "\n",
        "    print(f\"Successfully created geometries: {successful_count} roads ({successful_count/total_unique_roads:.2%})\")\n",
        "    print(f\"Failed to create geometries: {failed_count} roads ({failed_count/total_unique_roads:.2%})\")\n",
        "\n",
        "    # Identify which specific roads failed\n",
        "    successful_names = set(result_gdf[name_column])\n",
        "    original_names = set(original_df[name_column])\n",
        "    failed_names = original_names - successful_names\n",
        "\n",
        "    if failed_names:\n",
        "        print(f\"\\nReturning a DataFrame with {len(failed_names)} failed records for review.\")\n",
        "        failed_df = original_df[original_df[name_column].isin(failed_names)].copy()\n",
        "        return failed_df\n",
        "    else:\n",
        "        print(\"\\nCongratulations! All road geometries were created successfully.\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "_AvT5gSQuwcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "road_df = unique_results[~unique_results['way_id'].isna()][['Road_Name', 'way_id', 'district']].drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "# Get the road route details\n",
        "road_route_details = extract_line_geometry(road_df, 'Road_Name','way_id').reset_index(drop=True)\n",
        "\n",
        "# Convert ID to int type\n",
        "road_route_details['way_id'] = road_route_details['way_id'].astype(int)\n",
        "\n",
        "# Inspect dataframe\n",
        "road_route_details.head()"
      ],
      "metadata": {
        "id": "-BJ9xfrNuw4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate test 2 summary\n",
        "generate_geometry_summary(road_route_details, road_df, 'Road_Name')"
      ],
      "metadata": {
        "id": "3FnMcPQDvCWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_mask = road_route_details['geometry'].isna()"
      ],
      "metadata": {
        "id": "2WZmY-dfvF8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 3"
      ],
      "metadata": {
        "id": "yBnz3iIPwQxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_district_data(object_id: str) -> dict | None:\n",
        "    \"\"\"\n",
        "    Fetches OSM data for a given object_id, trying as relation, then way, then node.\n",
        "    Correctly handles inner and outer ways for relations to form polygons with holes.\n",
        "    \"\"\"\n",
        "    final_object_type = None\n",
        "    final_tags = {}\n",
        "    final_name_tag = None\n",
        "    final_all_polygons_coordinates = []\n",
        "    processed_successfully = False\n",
        "\n",
        "    # --- Try to process as a RELATION ---\n",
        "    try:\n",
        "        relation_url = f\"{OSM_API_BASE_URL}/relation/{object_id}/full\"\n",
        "        root_xml = fetch_osm_data(relation_url)\n",
        "        if not root_xml:\n",
        "            raise ValueError(\"XML data could not be fetched.\")\n",
        "\n",
        "        relation_element = root_xml.find(f\".//relation[@id='{object_id}']\")\n",
        "        if relation_element is None:\n",
        "            raise ValueError(\"Relation element not found in XML.\")\n",
        "\n",
        "        # (This part for getting tags and caching node coordinates is correct and remains the same)\n",
        "        current_tags = {tag.get('k'): tag.get('v') for tag in relation_element.findall('tag') if tag.get('k')}\n",
        "        current_name_tag = current_tags.get('name')\n",
        "        nodes_coords_cache = {\n",
        "            node.get('id'): (float(node.get('lon')), float(node.get('lat')))\n",
        "            for node in root_xml.findall('.//node') if node.get('id') and node.get('lat') and node.get('lon')\n",
        "        }\n",
        "\n",
        "        # (This part for separating ways into outer/inner segments is also correct)\n",
        "        outer_way_segments = []\n",
        "        inner_way_segments = []\n",
        "        for member in relation_element.findall(\"member[@type='way']\"):\n",
        "            way_elem = root_xml.find(f\".//way[@id='{member.get('ref')}']\")\n",
        "            if way_elem is not None:\n",
        "                coords = [nodes_coords_cache[nd.get('ref')] for nd in way_elem.findall('nd') if nd.get('ref') in nodes_coords_cache]\n",
        "                if coords:\n",
        "                    role = member.get('role', 'outer')\n",
        "                    if role == 'outer':\n",
        "                        outer_way_segments.append(coords)\n",
        "                    elif role == 'inner':\n",
        "                        inner_way_segments.append(coords)\n",
        "\n",
        "        # --- DELETED SECTION: The original, fragile stitching loops have been removed. ---\n",
        "\n",
        "        # +++ NEW, ROBUST STITCHING AND POLYGON CREATION LOGIC +++\n",
        "\n",
        "        # Convert coordinate segments into Shapely LineString objects\n",
        "        outer_lines = [LineString(segment) for segment in outer_way_segments]\n",
        "        inner_lines = [LineString(segment) for segment in inner_way_segments]\n",
        "\n",
        "        # Merge all connecting lines into continuous, single paths\n",
        "        merged_outer_lines = unary_union(outer_lines)\n",
        "        merged_inner_lines = unary_union(inner_lines)\n",
        "\n",
        "        # Form valid polygons from the closed rings created by the merged lines\n",
        "        stitched_outer_polygons = list(polygonize(merged_outer_lines))\n",
        "        stitched_inner_polygons = list(polygonize(merged_inner_lines))\n",
        "\n",
        "        # --- RESUMING LOGIC WITH CORRECTLY FORMED POLYGONS ---\n",
        "\n",
        "        final_shapely_polygons = []\n",
        "        # Create a mutable list of inner polygons to track which ones have been used\n",
        "        remaining_inners = list(stitched_inner_polygons)\n",
        "\n",
        "        for outer_poly in stitched_outer_polygons:\n",
        "            holes_for_this_poly = []\n",
        "            # This list will hold inner polygons that haven't been assigned to this outer_poly\n",
        "            unassigned_inners = []\n",
        "\n",
        "            for inner_poly in remaining_inners:\n",
        "                # Check if the inner polygon is properly contained within the outer one\n",
        "                if outer_poly.contains(inner_poly):\n",
        "                    holes_for_this_poly.append(inner_poly.exterior.coords)\n",
        "                else:\n",
        "                    unassigned_inners.append(inner_poly)\n",
        "\n",
        "            # Update the list of remaining inners for the next outer polygon\n",
        "            remaining_inners = unassigned_inners\n",
        "\n",
        "            # Create the final polygon with its associated holes\n",
        "            final_shapely_polygons.append(Polygon(outer_poly.exterior.coords, holes_for_this_poly))\n",
        "\n",
        "        # --- (The rest of the function for formatting output remains the same) ---\n",
        "\n",
        "        if final_shapely_polygons:\n",
        "            for shp_poly in final_shapely_polygons:\n",
        "                exterior_coords = list(shp_poly.exterior.coords)\n",
        "                if not shp_poly.exterior.is_ccw:\n",
        "                    exterior_coords.reverse()\n",
        "                poly_data = [exterior_coords]\n",
        "                for interior_ring in shp_poly.interiors:\n",
        "                    interior_coords = list(interior_ring.coords)\n",
        "                    if interior_ring.is_ccw:\n",
        "                        interior_coords.reverse()\n",
        "                    poly_data.append(interior_coords)\n",
        "                final_all_polygons_coordinates.append(poly_data)\n",
        "\n",
        "            final_tags = current_tags\n",
        "            final_name_tag = current_name_tag\n",
        "            final_object_type = \"relation\"\n",
        "            processed_successfully = True\n",
        "            print(f\"Successfully processed ID {object_id} as RELATION with {len(final_shapely_polygons)} polygon(s).\")\n",
        "        else:\n",
        "            print(f\"INFO: Relation {object_id} could not form valid polygons.\")\n",
        "\n",
        "    except Exception as e_relation:\n",
        "        print(f\"Error processing relation for {object_id}: {e_relation}. Trying as way.\")\n",
        "        processed_successfully = False\n",
        "\n",
        "    # (The fallback logic for 'WAY' and 'NODE' remains unchanged)\n",
        "    # ...\n",
        "\n",
        "    # --- Final Return ---\n",
        "    if not processed_successfully or not final_all_polygons_coordinates:\n",
        "        print(f\"FINAL: Could not derive usable geometry for OSM object ID {object_id}.\")\n",
        "        return None\n",
        "\n",
        "    return {\n",
        "        'name': final_name_tag,\n",
        "        'tags': final_tags,\n",
        "        'all_polygons_coordinates': final_all_polygons_coordinates,\n",
        "        'id': object_id,\n",
        "        'type': final_object_type\n",
        "    }"
      ],
      "metadata": {
        "id": "0rY0Up0b5vN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_geometry(row):\n",
        "    coords_list = row['all_polygons_coordinates'] # This will be a list of lists of coords\n",
        "    if not coords_list:\n",
        "        return None\n",
        "\n",
        "    if row['type'] == 'node':\n",
        "        # For a node, coords_list will contain one list with one (lon, lat) tuple: [[(lon, lat)]]\n",
        "        if coords_list and len(coords_list[0]) == 1:\n",
        "            return Point(coords_list[0][0])\n",
        "        return None\n",
        "\n",
        "    #\n",
        "    elif row['type'] == 'way_polygon':\n",
        "        if coords_list and coords_list[0] and len(coords_list[0]) >= 4:\n",
        "            if coords_list[0][0] != coords_list[0][-1]:\n",
        "                print(f\"Warning: way_polygon {row['id']} was not closed, closing it now.\")\n",
        "                return Polygon(coords_list[0] + [coords_list[0][0]])\n",
        "            return Polygon(coords_list[0])\n",
        "        else:\n",
        "            print(f\"Warning: way_polygon {row['id']} has insufficient points ({len(coords_list[0])}). Cannot form Polygon.\")\n",
        "            return None\n",
        "        return None\n",
        "\n",
        "    elif row['type'] == 'way_line':\n",
        "        if coords_list and coords_list[0] and len(coords_list[0]) >= 2:\n",
        "            return LineString(coords_list[0])\n",
        "        else:\n",
        "            print(f\"Warning: way_line {row['id']} has insufficient points ({len(coords_list[0])}). Cannot form LineString.\")\n",
        "            return None\n",
        "        return None\n",
        "\n",
        "    elif row['type'] == 'relation':\n",
        "        polygons = []\n",
        "        for poly_data in coords_list:\n",
        "            if poly_data:\n",
        "                exterior_coords = poly_data[0]\n",
        "                interior_coords_list = poly_data[1:]\n",
        "\n",
        "                # Create Shapely Polygon with holes\n",
        "                try:\n",
        "                    poly = Polygon(exterior_coords, interior_coords_list)\n",
        "                    if poly.is_valid:\n",
        "                        polygons.append(poly)\n",
        "                    else:\n",
        "                        print(f\"Warning: Invalid polygon created for relation {row['id']}. Attempting to make valid.\")\n",
        "                        valid_poly = poly.buffer(0)\n",
        "                        if valid_poly.is_valid:\n",
        "                            if isinstance(valid_poly, MultiPolygon):\n",
        "                                polygons.extend(valid_poly.geoms)\n",
        "                            else:\n",
        "                                polygons.append(valid_poly)\n",
        "                        else:\n",
        "                            print(f\"Warning: Could not make polygon valid for relation {row['id']}.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error creating polygon for relation {row['id']}: {e}\")\n",
        "\n",
        "        if polygons:\n",
        "            return MultiPolygon(polygons) if len(polygons) > 1 else polygons[0]\n",
        "        return None\n",
        "\n",
        "    else:\n",
        "        print(f\"Unsupported or old type '{row['type']}'. Skipping geometry creation.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def create_amen_gdf(df, id_column):\n",
        "    gdf_list = []\n",
        "    df[id_column] = df[id_column].astype(str)\n",
        "\n",
        "    # Fetch data for each ID and append to gdf_list\n",
        "    for osm_id in df[id_column].unique(): # Use unique IDs to avoid redundant API calls\n",
        "        result = get_district_data(osm_id)\n",
        "        if result:\n",
        "            gdf_list.append(result)\n",
        "\n",
        "    if not gdf_list:\n",
        "        print(\"No valid district data was fetched.\")\n",
        "        return None\n",
        "\n",
        "    # Create a DataFrame from the fetched OSM data\n",
        "    dff = pd.DataFrame(gdf_list)\n",
        "\n",
        "    # Merge the original DataFrame with the fetched OSM data\n",
        "    # Use left_on=id_column, right_on='id' to merge correctly\n",
        "    merge_df = pd.merge(df, dff, left_on=id_column, right_on='id', how='left')\n",
        "\n",
        "    # Create geometry column\n",
        "    merge_df['geometry'] = merge_df.apply(create_geometry, axis=1)\n",
        "\n",
        "    # Filter out rows where geometry could not be created\n",
        "    merge_df = merge_df[merge_df['geometry'].notnull()].reset_index(drop=True)\n",
        "\n",
        "    # Create GeoDataFrame\n",
        "    gdf = gpd.GeoDataFrame(merge_df, geometry='geometry', crs=\"EPSG:4326\")\n",
        "    return gdf"
      ],
      "metadata": {
        "id": "8EzulcPgwCK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test3_ready = road_route_details[~missing_mask]"
      ],
      "metadata": {
        "id": "OdxKE-rxwFEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Execution ---\n",
        "district_osm_dict = {\n",
        "    'GOMBAK': '12438352',\n",
        "    'HULU LANGAT': '12438351',\n",
        "    'KLANG': '12391135',\n",
        "    'HULU SELANGOR': '10714199',\n",
        "    'KUALA LANGAT': '10743362',\n",
        "    'KUALA LUMPUR': '2939672',\n",
        "    'KUALA SELANGOR': '10714137',\n",
        "    'PETALING': '12391134',\n",
        "    'PUTRAJAYA': '4443881',\n",
        "    'SABAK BERNAM': '10714136',\n",
        "    'SEPANG': '10743315'\n",
        "}\n",
        "\n",
        "district_df = pd.DataFrame.from_dict(district_osm_dict, orient='index', columns=['id'])\n",
        "district_df.index.name = 'district'\n",
        "district_df = district_df.reset_index()\n",
        "district_df['id'] = district_df['id'].astype(str)\n",
        "\n",
        "display(district_df.head())"
      ],
      "metadata": {
        "id": "stlaskN9wYeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dis = create_amen_gdf(district_df, 'id')\n",
        "dis"
      ],
      "metadata": {
        "id": "b-8I8awQ55dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_record_locations(\n",
        "    records_gdf: gpd.GeoDataFrame,\n",
        "    districts_gdf: gpd.GeoDataFrame,\n",
        "    district_col: str = 'district'\n",
        ") -> gpd.GeoDataFrame:\n",
        "    \"\"\"\n",
        "    Finds the actual district for each road (LineString) based on maximum length overlap.\n",
        "\n",
        "    Args:\n",
        "        records_gdf (gpd.GeoDataFrame): Roads to check. Must have LineString geometry and `district_col`.\n",
        "        districts_gdf (gpd.GeoDataFrame): District boundaries with Polygon geometry.\n",
        "        district_col (str): The column name linking records to districts.\n",
        "\n",
        "    Returns:\n",
        "        gpd.GeoDataFrame: Original records with 'actual_district', 'is_in_correct_district',\n",
        "                          and 'overlap_length' columns.\n",
        "    \"\"\"\n",
        "    # Validation and Preparation\n",
        "    if records_gdf.empty or districts_gdf.empty:\n",
        "        print(\"Warning: One or both GeoDataFrames are empty.\")\n",
        "        records_gdf['actual_district'] = 'N/A'\n",
        "        records_gdf['is_in_correct_district'] = False\n",
        "        return records_gdf\n",
        "\n",
        "    # Ensure both GeoDataFrames use the same CRS\n",
        "    if records_gdf.crs != districts_gdf.crs:\n",
        "        print(f\"Warning: CRS mismatch. Reprojecting records_gdf to match districts_gdf.\")\n",
        "        records_gdf = records_gdf.to_crs(districts_gdf.crs)\n",
        "\n",
        "    # Validate and Fix District Geometries\n",
        "    districts_clean = districts_gdf.copy()\n",
        "    districts_clean['geometry'] = districts_clean['geometry'].buffer(0)\n",
        "\n",
        "    # Find District with Maximum Road Length Overlap\n",
        "    results = []\n",
        "\n",
        "    for idx, record in records_gdf.iterrows():\n",
        "        road_geom = record.geometry\n",
        "        assigned_district = record[district_col]\n",
        "\n",
        "        best_district = None\n",
        "        max_overlap_length = 0\n",
        "        overlap_details = {}\n",
        "\n",
        "        # Check intersection with each district\n",
        "        for _, district in districts_clean.iterrows():\n",
        "            district_geom = district.geometry\n",
        "            district_name = district[district_col]\n",
        "\n",
        "            if district_geom.intersects(road_geom):\n",
        "                # Calculate the length of road within this district\n",
        "                intersection = district_geom.intersection(road_geom)\n",
        "\n",
        "                # Handle different intersection result types\n",
        "                if intersection.is_empty:\n",
        "                    overlap_length = 0\n",
        "                elif hasattr(intersection, 'length'):\n",
        "                    overlap_length = intersection.length\n",
        "                else:\n",
        "                    # Handle GeometryCollection or MultiLineString\n",
        "                    overlap_length = sum(\n",
        "                        geom.length for geom in intersection.geoms\n",
        "                        if hasattr(geom, 'length')\n",
        "                    )\n",
        "\n",
        "                overlap_details[district_name] = overlap_length\n",
        "\n",
        "                # Track the district with maximum overlap\n",
        "                if overlap_length > max_overlap_length:\n",
        "                    max_overlap_length = overlap_length\n",
        "                    best_district = district_name\n",
        "\n",
        "        # Build result record\n",
        "        record_dict = record.to_dict()\n",
        "        record_dict['actual_district'] = best_district if best_district else 'Outside any district'\n",
        "        record_dict['is_in_correct_district'] = (assigned_district == best_district) if best_district else False\n",
        "        record_dict['overlap_length'] = max_overlap_length\n",
        "        record_dict['total_road_length'] = road_geom.length\n",
        "        record_dict['overlap_percentage'] = (max_overlap_length / road_geom.length * 100) if road_geom.length > 0 else 0\n",
        "\n",
        "        results.append(record_dict)\n",
        "\n",
        "    # Create Result GeoDataFrame\n",
        "    validated_gdf = gpd.GeoDataFrame(results, crs=records_gdf.crs)\n",
        "\n",
        "    return validated_gdf"
      ],
      "metadata": {
        "id": "irNvpKDfwZ4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "districts_gdf = gpd.GeoDataFrame(dis, crs=\"EPSG:4326\")\n",
        "input_tprop = road_route_details.copy()\n",
        "records_gdf = gpd.GeoDataFrame(input_tprop, crs=\"EPSG:4326\")"
      ],
      "metadata": {
        "id": "OKUqD-Bewbc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validated_records = validate_record_locations(records_gdf, districts_gdf)"
      ],
      "metadata": {
        "id": "GjKO02TWwkK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validated_df = df.merge(validated_records, on =['road_name', 'way_id'], how = 'left')\n",
        "validated_df"
      ],
      "metadata": {
        "id": "d_n17IWTx3be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#validated_df.to_excel('/content/drive/MyDrive/Colab/Capstone 1/assessed.xlsx')"
      ],
      "metadata": {
        "id": "YKJXDXmDwrZ4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}